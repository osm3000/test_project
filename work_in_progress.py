# -*- coding: utf-8 -*-
"""work_in_progress.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19eJQgElzihPChQpxjJpWo8qLAqT6IT6n
"""
import os
import pandas as pd
import numpy as np
import torch
from torch import nn
from sentence_transformers import SentenceTransformer
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import train_test_split
import pytorch_lightning as pl
from torch.utils.data import DataLoader, Dataset
from torch.utils.data import TensorDataset
from pytorch_lightning.callbacks.early_stopping import EarlyStopping
from pytorch_lightning.callbacks import QuantizationAwareTraining
import torch.nn.functional as F
from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score
from torchinfo import summary


import warnings
from sklearn.exceptions import DataConversionWarning, UndefinedMetricWarning
warnings.filterwarnings(action='ignore', category=UndefinedMetricWarning)

training_configs = {
    'max_epochs': 1000,
    # 'batch_size': 16,
    'batch_size': 512,
    'average_epochs': 0,
    'backend': "qnnpack",
    # 'learning_rate': 0.00001
    'learning_rate': 0.001
}

# # set the qconfig for PTQ
# qconfig = torch.quantization.get_default_qconfig(training_configs['backend'])
# # or, set the qconfig for QAT
# qconfig = torch.quantization.get_default_qat_qconfig(training_configs['backend'])
# # set the qengine to control weight packing
# torch.backends.quantized.engine = training_configs['backend']

data=pd.read_csv('./data/thought_examples.csv')

create_new_embeddings = False

data_training = None
if create_new_embeddings:
    sbert_model = SentenceTransformer('all-mpnet-base-v2')
    text_embedding=sbert_model.encode(data['text input'], show_progress_bar=True)

    data_training=pd.DataFrame(text_embedding)
    data_training.to_json("./data/embedded_data.json")
else:
    with open("./data/embedded_data.json", 'r') as file_handle:
        data_training = pd.read_json(file_handle)
mapper = {x:i for i,x in enumerate(data['category'].unique())}
Y = data['category'].map(lambda x: mapper[x]).values
X = data_training.values

class thought_model(nn.Sequential):
    def __init__(self):
        super().__init__(nn.Linear(768, 30),
        nn.Dropout(0.3),
        nn.GELU(),
        nn.Linear(30, 2),
        nn.GELU())
        
        for m in self.modules():
            if isinstance(m, nn.Linear):
                torch.nn.init.xavier_uniform_(m.weight)

    
class net_trainer(pl.LightningModule):
    
    def __init__(self,model):
        super().__init__()
        self.model = model
        self.model_validation_data = []
        self.total_nb_of_epochs = 0
    
    def forward(self, x):
        output = self.model(x)
        return output

    def training_step(self, batch, batch_idx):       
        scores, target = batch
        output = self.model(scores)
        loss = F.cross_entropy(output,target) 
        self.log('train_loss', loss)
        return loss
    
    def validation_step(self, batch, batch_idx):
        scores, target = batch
        output = self.model(scores)
        loss = F.cross_entropy(output,target) 
        self.log('val_loss', loss)
        self.total_nb_of_epochs += 1
        return loss, output, target
    
    def validation_epoch_end(self, validation_step_outputs):
        loss, output, target = validation_step_outputs[0]
        pred = F.softmax(output, dim=-1)
        result = torch.argmax(pred, dim = -1)
        target = target.cpu()
        result = result.cpu()
        f1_score_val = f1_score(target, result, average = 'macro')
        acc_val = accuracy_score(target, result)
        recall_val = recall_score(target, result, average = 'macro')
        precision_val = precision_score(target, result, average = 'macro')
        

        self.model_validation_data.append([f1_score_val, acc_val, recall_val, precision_val])
        self.log('val_f1', f1_score_val)
        # print('LOSS :', loss.item() ,'F1 score : ',f1_score_val , '|ACC :',acc_val, '|RECALL : ',recall_val, '|PRECISION : ',precision_val)

    def configure_optimizers(self):
        global training_configs
        optimizer = torch.optim.Adam(self.parameters(), lr=training_configs['learning_rate'])
        return optimizer
    
    def get_history(self):
        return pd.DataFrame(self.model_validation_data)
    
    
def create_dataset(X,y, batch_size = 10):
    dataset_ = TensorDataset(torch.as_tensor(X).float(),torch.tensor(y).long())
    loader = DataLoader(dataset_, batch_size = batch_size, pin_memory=True, shuffle=True)
    return dataset_, loader

def perform_cv_model():
    global training_configs
    skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=1)
    f1_score_folds=[]
    accuracy_folds=[]

    f1_score_folds_dynamic_int8=[]
    accuracy_folds_dynamic_int8=[]
    
    f1_score_folds_static_int8=[]
    accuracy_folds_static_int8=[]

    nb_of_epochs = []

    for train_index, test_index in skf.split(X, Y):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = Y[train_index], Y[test_index]
        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42, stratify = y_train)
        trainset, trainloader = create_dataset(X_train, y_train, batch_size=training_configs['batch_size'])
        valset, valloader = create_dataset(X_val, y_val, batch_size = len(y_val))
        early_stop_callback = EarlyStopping(
            monitor='val_loss',
            min_delta=0.00,
            patience=10,
            verbose=False,
            mode='min'
            )
        
        model = thought_model()
        net_system = net_trainer(model)
        # trainer = pl.Trainer(max_epochs=500, callbacks=[QuantizationAwareTraining(qconfig='qnnpack'), early_stop_callback])
        trainer = pl.Trainer(max_epochs=training_configs['max_epochs'], callbacks=[early_stop_callback])
    

        # trainer = Trainer(accelerator="gpu", devices=1)
        trainer.fit(net_system, trainloader,valloader)

        model.eval() # to properly use the drop-out


        nb_of_epochs.append(net_system.total_nb_of_epochs)

        pred = torch.argmax(torch.softmax(net_system(torch.tensor(X_test).float()), 1), 1).detach().numpy()
        # pred = torch.argmax(torch.softmax(net_system.dequant(torch.tensor(X_test).float()),1),1).detach().numpy()
        
        f1_score_folds.append(f1_score(y_test,pred, average = 'macro'))
        accuracy_folds.append(accuracy_score(y_test,pred))

        print(f"This split results: F1 score {f1_score_folds[-1]}, Accuracy: {accuracy_folds[-1]}")

        # model.eval()
        
        model_dynamic_int8 = get_dynamic_discretization(original_model=model)

        pred = torch.argmax(torch.softmax(model_dynamic_int8(torch.tensor(X_test).float()),1),1).detach().numpy()
        # pred = torch.argmax(torch.softmax(model_dynamic_int8(torch.tensor(X_test).float()),1),1).detach().numpy()
        
        f1_score_folds_dynamic_int8.append(f1_score(y_test,pred, average = 'macro'))
        accuracy_folds_dynamic_int8.append(accuracy_score(y_test,pred))

        print(f"This dynamic int-8 split results: F1 score{f1_score_folds_dynamic_int8[-1]}, Accuracy: {accuracy_folds_dynamic_int8[-1]}")

        model_static_quantized = get_static_discretization(original_model=model)

        pred = torch.argmax(torch.softmax(model_static_quantized(torch.tensor(X_test).float()), 1), 1).detach().numpy()

        f1_score_folds_static_int8.append(f1_score(y_test, pred, average='macro'))
        accuracy_folds_static_int8.append(accuracy_score(y_test, pred))

        print(f"This static int-8 split results: F1 score{f1_score_folds_static_int8[-1]}, Accuracy: {accuracy_folds_static_int8[-1]}")

    training_configs['average_epochs'] = int(np.mean(nb_of_epochs))

    print(np.mean(f1_score_folds), np.mean(f1_score_folds_dynamic_int8), np.mean(f1_score_folds_static_int8))
    print(np.mean(accuracy_folds), np.mean(accuracy_folds_dynamic_int8), np.mean(accuracy_folds_static_int8))
    print(np.mean(nb_of_epochs))
    print("----------------------------------")

def train_final_model():
    '''
    Train the final model to be deployed on the whole data
    - 
    '''
    global training_configs
    trainset, trainloader = create_dataset(X, Y, batch_size=training_configs['batch_size'])

    model = thought_model()
    net_system = net_trainer(model)
    # trainer = pl.Trainer(max_epochs=500, callbacks=[QuantizationAwareTraining(qconfig='qnnpack'), early_stop_callback])
    trainer = pl.Trainer(max_epochs=training_configs['average_epochs'])


    # trainer = Trainer(accelerator="gpu", devices=1)
    trainer.fit(net_system, trainloader)

    model.eval()

    print(f'Size of full model: {print_model_size(model)}')
    model_static_quantized = get_static_discretization(original_model=model)
    print(f'Size of static model: {print_model_size(model_static_quantized)}')

    model_dynamic_int8 = get_dynamic_discretization(original_model=model)
    print(f'Size of dynamic model: {print_model_size(model_dynamic_int8)}')




    model_scripted = torch.jit.script(model)  # Export to TorchScript
    model_scripted.save('./saved_models/normal_full_data_model.pt') # Save

    model_scripted = torch.jit.script(model_dynamic_int8)  # Export to TorchScript
    model_scripted.save('./saved_models/dynamic_quant_full_data_model.pt')  # Save
    
    model_scripted = torch.jit.script(model_static_quantized)  # Export to TorchScript
    model_scripted.save('./saved_models/static_quant_full_data_model.pt')  # Save

    print(f"Model is trained. Mission Accomplished.")
    # pred = torch.argmax(torch.softmax(net_system(torch.tensor(X_test).float()), 1), 1).detach().numpy()
    # pred = torch.argmax(torch.softmax(net_system.dequant(torch.tensor(X_test).float()),1),1).detach().numpy()

def get_dynamic_discretization(original_model):

    # set the qconfig for PTQ
    qconfig = torch.quantization.get_default_qconfig(training_configs['backend'])
    # or, set the qconfig for QAT
    qconfig = torch.quantization.get_default_qat_qconfig(
        training_configs['backend'])
    # set the qengine to control weight packing
    torch.backends.quantized.engine = training_configs['backend']

    return torch.quantization.quantize_dynamic(
        original_model,  # the original model
        {torch.nn.Linear},  # a set of layers to dynamically quantize
        dtype=torch.qint8)  # the target dtype for quantized weights

def get_static_discretization(original_model):
    # set the qconfig for PTQ
    qconfig = torch.quantization.get_default_qconfig(training_configs['backend'])
    # or, set the qconfig for QAT
    qconfig = torch.quantization.get_default_qat_qconfig(
        training_configs['backend'])
    # set the qengine to control weight packing
    torch.backends.quantized.engine = training_configs['backend']


    model_static_quantized = torch.quantization.prepare(
        original_model, inplace=False)
    model_static_quantized = torch.quantization.convert(model_static_quantized, inplace=False)
    return model_static_quantized


def print_model_size(mdl):
    '''
    Taken from https://pytorch.org/tutorials/recipes/quantization.html
    '''
    torch.save(mdl.state_dict(), "tmp.pt")
    print("%.2f MB" % (os.path.getsize("tmp.pt")/1e6))
    os.remove('tmp.pt')

if __name__ == '__main__':
    perform_cv_model()
    train_final_model()
